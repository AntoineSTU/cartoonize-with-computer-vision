{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOjEnL5xFi5y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "# import imutils\n",
        "from tqdm import tqdm\n",
        "from abc import ABC, abstractmethod\n",
        "from nptyping import NDArray\n",
        "from typing import Any, Tuple, List, Union, Optional\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "# sys.path.append(os.path.join(\"..\", \"..\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ImageType = Union[NDArray[(Any, Any), np.uint8], NDArray[(Any, Any, 2), np.uint8], NDArray[(Any, Any, 3), np.uint8], NDArray[(Any, Any, 4), np.uint8]]\n",
        "\n",
        "\n",
        "class ImageFormat(Enum):\n",
        "    \"\"\"Image format\"\"\"\n",
        "\n",
        "    BLACK_AND_WHITE = \"BLACK_AND_WHITE\"\n",
        "    BLACK_AND_WHITE_WITH_TRANSPARENCY = \"BLACK_AND_WHITE_WITH_TRANSPARENCY\"\n",
        "    COLORED = \"COLORED\"\n",
        "    COLORED_WITH_TRANSPARENCY = \"COLORED_WITH_TRANSPARENCY\"\n",
        "\n",
        "def format_image(image: ImageType, to_format) -> Tuple[ImageType, Optional[NDArray[(Any, Any), np.uint8]]]:\n",
        "    \"\"\"To format a nD image into a mD one\"\"\"\n",
        "    if to_format == ImageFormat.BLACK_AND_WHITE:\n",
        "        if image.ndim == 2:\n",
        "            return image,\n",
        "        if image.ndim == 3 and image.shape[-1] == 2:\n",
        "            mask = image[:, :, 1]\n",
        "            image = image[:, :, 0]\n",
        "            return image, mask\n",
        "        if image.ndim == 3 and image.shape[-1] == 3:\n",
        "            return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY),\n",
        "        if image.ndim == 3 and image.shape[-1] == 4:\n",
        "            mask = image[:, :, 3],\n",
        "            image = cv2.cvtColor(image[:, :, :3], cv2.COLOR_RGB2GRAY)\n",
        "            return image, mask\n",
        "    if to_format == ImageFormat.BLACK_AND_WHITE_WITH_TRANSPARENCY:\n",
        "        if image.ndim == 2:\n",
        "            image_with_transparency = np.zeros((image.shape[0], image.shape[1], 2), dtype=np.uint8)\n",
        "            mask = 255 * np.ones((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "            image_with_transparency[:, :, 0] = image\n",
        "            image_with_transparency[:, :, 1] = mask\n",
        "            return image, mask\n",
        "        if image.ndim == 3 and image.shape[-1] == 2:\n",
        "            return image, image[:, :, 3]\n",
        "        if image.ndim == 3 and image.shape[-1] == 3:\n",
        "            image_with_transparency = np.zeros((image.shape[0], image.shape[1], 2), dtype=np.uint8)\n",
        "            mask = 255 * np.ones((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "            image_with_transparency[:, :, 0] = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "            image_with_transparency[:, :, 1] = mask\n",
        "            return image_with_transparency, mask\n",
        "        if image.ndim == 3 and image.shape[-1] == 4:\n",
        "            image_with_transparency = np.zeros((image.shape[0], image.shape[1], 2), dtype=np.uint8)\n",
        "            mask = image[:, :, 3],\n",
        "            image_with_transparency[:, :, 0] = cv2.cvtColor(image[:, :, :3], cv2.COLOR_RGB2GRAY)\n",
        "            image_with_transparency[:, :, 1] = mask\n",
        "            return image, mask\n",
        "    if to_format == ImageFormat.COLORED:\n",
        "        if image.ndim == 2:\n",
        "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB),\n",
        "        if image.ndim == 3 and image.shape[-1] == 2:\n",
        "            mask = image[:, :, 1]\n",
        "            image = image[:, :, 0]\n",
        "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB), mask\n",
        "        if image.ndim == 3 and image.shape[-1] == 3:\n",
        "            return image,\n",
        "        if image.ndim == 3 and image.shape[-1] == 4:\n",
        "            mask = image[:, :, 3],\n",
        "            image = image[:, :, :3]\n",
        "            return image, mask\n",
        "    if to_format == ImageFormat.COLORED_WITH_TRANSPARENCY:\n",
        "        if image.ndim == 2:\n",
        "            image_with_transparency = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
        "            mask = 255 * np.ones((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "            image_with_transparency[:, :, :3] = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            image_with_transparency[:, :, 3] = mask\n",
        "            return image_with_transparency, mask\n",
        "        if image.ndim == 3 and image.shape[-1] == 2:\n",
        "            image_with_transparency = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
        "            mask = image[:, :, 1]\n",
        "            image_with_transparency[:, :, :3] = cv2.cvtColor(image[:, :, 0], cv2.COLOR_GRAY2RGB)\n",
        "            image_with_transparency[:, :, 3] = mask\n",
        "            return image_with_transparency, mask\n",
        "        if image.ndim == 3 and image.shape[-1] == 3:\n",
        "            image_with_transparency = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
        "            mask = 255 * np.ones((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "            image_with_transparency[:, :, :3] = image\n",
        "            image_with_transparency[:, :, 3] = mask\n",
        "            return image_with_transparency, mask\n",
        "        if image.ndim == 3 and image.shape[-1] == 4:\n",
        "            return image, image[:, :, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8_mLkfcFwXL"
      },
      "outputs": [],
      "source": [
        "class Transformer(ABC):\n",
        "    \"\"\"Generic image transformation\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "\n",
        "class Combiner(ABC):\n",
        "    \"\"\"Generic image combiner\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def combine(self, input_img1: ImageType, input_img2: ImageType) -> ImageType:\n",
        "        \"\"\"Combine two images\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvcIGOAaIi5h"
      },
      "outputs": [],
      "source": [
        "class GaussianBlurTransformer(Transformer):\n",
        "    \"\"\"Apply gaussian blur to transformer\"\"\"\n",
        "\n",
        "    def __init__(self, kernel: Tuple[int, int] = (5, 5), stdev: int = 0):\n",
        "        \"\"\"Initialize the blur parameters\"\"\"\n",
        "        self.kernel = kernel\n",
        "        self.stdev = stdev\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        return cv2.GaussianBlur(input_img, self.kernel, self.stdev)\n",
        "\n",
        "class MedianBlurTransformer(Transformer):\n",
        "    \"\"\"Apply median blur to transformer\"\"\"\n",
        "\n",
        "    def __init__(self, ksize: int = 5):\n",
        "        \"\"\"Initialize the blur parameters\"\"\"\n",
        "        self.ksize = ksize\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        return cv2.medianBlur(input_img, self.ksize)\n",
        "\n",
        "class BilateralBlurTransformer(Transformer):\n",
        "    \"\"\"Apply bilateral blur to transformer\"\"\"\n",
        "\n",
        "    def __init__(self, sigma_color: int = 5, sigma_space: int = 80, border_type: int = 80):\n",
        "        \"\"\"Initialize the blur parameters\"\"\"\n",
        "        self.sigma_color = sigma_color\n",
        "        self.sigma_space = sigma_space\n",
        "        self.border_type = border_type\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        return cv2.bilateralFilter(input_img, self.sigma_color, self.sigma_space, self.border_type)\n",
        "\n",
        "class CannyEdgeTransformer(Transformer):\n",
        "    \"\"\"Apply canny edge detection\"\"\"\n",
        "\n",
        "    def __init__(self, th_min: int = 30, th_max: int = 150):\n",
        "        \"\"\"Initialize the canny edge detector\"\"\"\n",
        "        self.th_min = th_min\n",
        "        self.th_max = th_max\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        return cv2.Canny(input_img, self.th_min, self.th_max)\n",
        "\n",
        "class DilateErodeTransformer(Transformer):\n",
        "    \"\"\"Delete noise by dilating then eroding\"\"\"\n",
        "\n",
        "    def __init__(self, kernel: Tuple[int, int] = (3, 3)):\n",
        "        \"\"\"Initialize the affine transformation\"\"\"\n",
        "        self.kernel = np.ones(kernel, np.uint8)\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        dilated = cv2.dilate(input_img, self.kernel, iterations=1)\n",
        "        return cv2.erode(dilated, self.kernel, iterations=1)\n",
        "\n",
        "class ContourTransformer(Transformer):\n",
        "    \"\"\"Draw contours of an image\"\"\"\n",
        "    def __init__(self, mode: int = cv2.RETR_TREE, method: int = cv2.CHAIN_APPROX_SIMPLE, edge_color: int = 0, edge_thickness: int = 1):\n",
        "        \"\"\"Initialize the contour transformer\"\"\"\n",
        "        self.mode = mode\n",
        "        self.method = method\n",
        "        self.edge_color = edge_color\n",
        "        self.edge_thickness = edge_thickness\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        contours, _ = cv2.findContours(input_img, self.mode, self.method)\n",
        "        # sort contours by area\n",
        "        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
        "        # create mask for drawing contours\n",
        "        mask = 255 * np.ones(input_img.shape, dtype=np.uint8)\n",
        "        # draw contours on mask\n",
        "        cv2.drawContours(mask, contours, contourIdx=-1, color=self.edge_color, thickness=self.edge_thickness)\n",
        "\n",
        "        # format\n",
        "        image_with_mask = np.zeros((input_img.shape[0], input_img.shape[1], 2), dtype=np.uint8)\n",
        "        image_with_mask[:, :, 0] = mask\n",
        "        image_with_mask[:, :, 1] = cv2.bitwise_not(mask)\n",
        "        return image_with_mask\n",
        "\n",
        "class AdaptiveThresholdContour(Transformer):\n",
        "    \"\"\"Draw contours thanks to adaptive threshold\"\"\"\n",
        "\n",
        "    def __init__(self, line_size: int = 3, blur_value: int = 3):\n",
        "        \"\"\"Initialize the adaptive threshold contour\"\"\"\n",
        "        self.line_size = line_size\n",
        "        self.blur_value = blur_value\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        edges = cv2.adaptiveThreshold(input_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, self.line_size, self.blur_value)\n",
        "        image_with_mask = np.zeros((input_img.shape[0], input_img.shape[1], 2), dtype=np.uint8)\n",
        "        image_with_mask[:, :, 0] = edges\n",
        "        image_with_mask[:, :, 1] = cv2.bitwise_not(edges)\n",
        "        return image_with_mask\n",
        "\n",
        "class ColorTransformer(Transformer):\n",
        "    \"\"\"To transform an image of a certain type to another\"\"\"\n",
        "    \n",
        "    def __init__(self, to_format: ImageFormat, return_mask: bool = False):\n",
        "        \"\"\"Initialize the image converter\"\"\"\n",
        "        self.to_format = to_format\n",
        "        self.return_mask = return_mask\n",
        "\n",
        "    def __call__(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        return self.transform(input_img)\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        idx = 1 if self.return_mask else 0\n",
        "        return format_image(input_img, self.to_format)[idx]\n",
        "\n",
        "class KmeansQuantizationTransformer(Transformer):\n",
        "    \"\"\"Quantize an image to a certain number of colors using k-means\"\"\"\n",
        "\n",
        "    def __init__(self, n_colors: int):\n",
        "        \"\"\"Initialize the color quantization transformer\"\"\"\n",
        "        self.n_colors = n_colors\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        data = np.float32(input_img).reshape((-1, 3))\n",
        "\n",
        "        # Determine criteria\n",
        "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 0.001)\n",
        "\n",
        "        # Implementing K-Means\n",
        "        _, label, center = cv2.kmeans(data, self.n_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "        center = np.uint8(center)\n",
        "        result = center[label.flatten()]\n",
        "        result = result.reshape(input_img.shape)\n",
        "        return result\n",
        "\n",
        "class BinsQuantizationTransformer(Transformer):\n",
        "    \"\"\"Quantize an image to a certain number of colors using bins\"\"\"\n",
        "\n",
        "    def __init__(self, n_colors: int):\n",
        "        \"\"\"Initialize the color quantization transformer\"\"\"\n",
        "        self.bins = 256//n_colors\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        return input_img // self.bins * self.bins + 128 // self.bins\n",
        "\n",
        "class HistogramEqualizationTransformer(Transformer):\n",
        "    \"\"\"Equalize the histogram of an image\"\"\"\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        return cv2.equalizeHist(input_img)\n",
        "\n",
        "class HistogramMatchingTransformer(Transformer):\n",
        "    \"\"\"Matches an images histogram with a given one\"\"\"\n",
        "\n",
        "    def __init__(self, histogram_path: str = \"reversed_histo.json\"):\n",
        "        \"\"\"Initialize the histogram matching transformer\"\"\"\n",
        "        with open(histogram_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.reverse_hists = json.load(f)\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        hsv = np.asarray(cv2.cvtColor(input_img, cv2.COLOR_RGB2HSV))\n",
        "        direct_hists = {\n",
        "            \"hue\": cv2.calcHist([hsv[:,:,0]],[0],None,[256],[0,256]).flatten(),\n",
        "            \"saturation\": cv2.calcHist([hsv[:,:,1]],[0],None,[256],[0,256]).flatten(),\n",
        "            \"value\": cv2.calcHist([hsv[:,:,2]],[0],None,[256],[0,256]).flatten()\n",
        "        }\n",
        "        final_hists = {}\n",
        "        for k, hist in direct_hists.items():\n",
        "            hists_cum = np.cumsum(hist)\n",
        "            hists_norm = (hists_cum/hists_cum[-1]*(len(hists_cum)-1)).astype(np.uint8)\n",
        "            # final_hists[k] = np.array([self.reverse_hists[k][hists_norm[i]] for i in range(len(hists_norm))])\n",
        "            final_hists[k] = np.array(self.reverse_hists[k])\n",
        "        final_img = np.zeros(hsv.shape)\n",
        "        # final_img[:,:,0] = cv2.LUT(hsv[:,:,0], final_hists[\"hue\"])\n",
        "        final_img[:,:,0] = hsv[:,:,0]\n",
        "        final_img[:,:,1] = cv2.LUT(hsv[:,:,1], final_hists[\"saturation\"])\n",
        "        final_img[:,:,2] = cv2.LUT(hsv[:,:,2], final_hists[\"value\"])\n",
        "        return cv2.cvtColor(final_img.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
        "\n",
        "class HSVAffineTransformer(Transformer):\n",
        "    \"\"\"Affine hsv transformation\"\"\"\n",
        "\n",
        "    def __init__(self, h_a: float = 1, h_b: float = 0, s_a: float = 1, s_b: float = 0, v_a: float = 1, v_b: float = 0):\n",
        "        \"\"\"Initialize the affine transformation (a*x + b)\"\"\"\n",
        "        self.h_a = h_a\n",
        "        self.h_b = h_b\n",
        "        self.s_a = s_a\n",
        "        self.s_b = s_b\n",
        "        self.v_a = v_a\n",
        "        self.v_b = v_b\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        hsv = cv2.cvtColor(input_img, cv2.COLOR_RGB2HSV)\n",
        "        hsv[:, :, 0] = (hsv[:, :, 0] * self.h_a + self.h_b) % 256\n",
        "        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * self.s_a + self.s_b, 0, 255)\n",
        "        hsv[:, :, 2] = np.clip(hsv[:, :, 2] * self.v_a + self.v_b, 0, 255)\n",
        "        return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "class SpecificColorTransformer(Transformer):\n",
        "    \"\"\"Modifines specicied colors.\"\"\"\n",
        "\n",
        "    def __init__(self, color_params: List[Tuple[List[int], float, float]]):\n",
        "        \"\"\"Initialize the specific color transformer\"\"\"\n",
        "        self.color_params = color_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MaskCombiner(Combiner):\n",
        "    \"\"\"Combine an image with a mask\"\"\"\n",
        "\n",
        "    def __init__(self, opaqueness: float = 1):\n",
        "        \"\"\"Initialize the mask combiner\"\"\"\n",
        "        self.opaqueness = opaqueness\n",
        "\n",
        "    def combine(self, image1: ImageType, image2_with_mask: ImageType) -> ImageType:\n",
        "        \"\"\"Combine two images\"\"\"\n",
        "        # return cv2.bitwise_and(input_img, input_img, mask=mask)\n",
        "\n",
        "        mask = image2_with_mask[:, :, -1]\n",
        "        image2 = image2_with_mask[:, :, :-1]\n",
        "\n",
        "        # get first masked value (foreground)\n",
        "        fg = cv2.bitwise_or(image2, image2, mask=mask)\n",
        "\n",
        "        # get second masked value (background) mask must be inverted\n",
        "        mask = cv2.bitwise_not(mask)\n",
        "        bk = cv2.bitwise_or(image1, image1, mask=mask)\n",
        "\n",
        "        # combine foreground+background\n",
        "        combined = cv2.bitwise_or(fg, bk)\n",
        "\n",
        "        # Use opaqueness\n",
        "        return (self.opaqueness * combined + (1 - self.opaqueness) * image1).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TransformerChain:\n",
        "    \"\"\"Defines a transformer chain\"\"\"\n",
        "    input_name: str\n",
        "    output_name: str\n",
        "    transformers: List[Transformer]\n",
        "\n",
        "@dataclass\n",
        "class CombinerType:\n",
        "    \"\"\"Defines a combiner chain\"\"\"\n",
        "    input_name1: str\n",
        "    input_name2: str\n",
        "    output_name: str\n",
        "    combiner: Combiner\n",
        "\n",
        "class PipelineTransformer(Transformer):\n",
        "    def __init__(self, actuators: List[Union[TransformerChain, CombinerType]]):\n",
        "        \"\"\"Initialize the global combiner\"\"\"\n",
        "        self.actuators = actuators\n",
        "\n",
        "    def transform(self, input_img: ImageType) -> ImageType:\n",
        "        \"\"\"Applies transform to an image\"\"\"\n",
        "        images = {\"input\": input_img}\n",
        "        for actuator in self.actuators:\n",
        "            if isinstance(actuator, TransformerChain):\n",
        "                image = actuator.transformers[0].transform(images[actuator.input_name])\n",
        "                for transformer in actuator.transformers[1:]:\n",
        "                    image = transformer.transform(image)\n",
        "                images[actuator.output_name] = image\n",
        "            elif isinstance(actuator, CombinerType):\n",
        "                images[actuator.output_name] = actuator.combiner.combine(images[actuator.input_name1], images[actuator.input_name2])\n",
        "        return images[\"output\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_image(image_path: str) -> np.ndarray:\n",
        "    \"\"\"Read image from path\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    assert image is not None, \"Image is None\"\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer_quantized_colors = TransformerChain(\n",
        "    input_name=\"input\",\n",
        "    output_name=\"output\",\n",
        "    transformers=[BinsQuantizationTransformer(n_colors=10), GaussianBlurTransformer(kernel=(7,7)), HistogramMatchingTransformer()]\n",
        ")\n",
        "transformer_chain_contours = TransformerChain(\n",
        "    input_name=\"input\",\n",
        "    output_name=\"contours\",\n",
        "    # transformers=[ColorTransformer(to_format=ImageFormat.BLACK_AND_WHITE), BlurTransformer(kernel=(7,7)), CannyEdgeTransformer(), DilateErodeTransformer(), BlurTransformer(), ContourTransformer(), ColorTransformer(to_format=ImageFormat.COLORED_WITH_TRANSPARENCY)]\n",
        "    transformers=[ColorTransformer(to_format=ImageFormat.BLACK_AND_WHITE), GaussianBlurTransformer(kernel=(7,7)), AdaptiveThresholdContour(), ColorTransformer(to_format=ImageFormat.COLORED_WITH_TRANSPARENCY)]\n",
        ")\n",
        "combiner_cartoon = CombinerType(\n",
        "    input_name1=\"quantized_colors\",\n",
        "    input_name2=\"contours\",\n",
        "    output_name=\"output\",\n",
        "    combiner=MaskCombiner(opaqueness=1)\n",
        ")\n",
        "# pipeline_cartoon = PipelineTransformer([transformer_quantized_colors, transformer_chain_contours, combiner_cartoon])\n",
        "pipeline_cartoon = PipelineTransformer([transformer_quantized_colors])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paths = [\"data/flickr/Images/667626_18933d713e.jpg\", \"data/flickr/Images/17273391_55cfc7d3d4.jpg\", \"data/flickr/Images/49553964_cee950f3ba.jpg\", \"data/flickr/Images/112243673_fd68255217.jpg\", \"data/flickr/Images/357725852_6f55cb9abc.jpg\"]\n",
        "\n",
        "for path in paths:\n",
        "    image = read_image(path)\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "    cartoon = pipeline_cartoon.transform(image)\n",
        "    plt.imshow(cartoon)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyGa2Oc7Fi5z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# cartoonize image with cv2\n",
        "def cartoonize_image(image):\n",
        "    \"\"\"Cartoonize image with cv2\"\"\"\n",
        "    # convert image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # apply gaussian blur\n",
        "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    # apply Canny edge detection\n",
        "    edges = cv2.Canny(blur, 30, 150)\n",
        "    # apply dilation and erosion to remove some noise\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    dilation = cv2.dilate(edges, kernel, iterations=1)\n",
        "    erosion = cv2.erode(dilation, kernel, iterations=1)\n",
        "    # apply blur to smooth out edges\n",
        "    blur = cv2.GaussianBlur(erosion, (5, 5), 0)\n",
        "    # find contours\n",
        "    contours, _ = cv2.findContours(blur, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # sort contours by area\n",
        "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
        "    # create mask for drawing contours\n",
        "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "    # draw contours on mask\n",
        "    cv2.drawContours(mask, contours, -1, (255, 255, 255), -1)\n",
        "    # apply mask to image\n",
        "    result = cv2.bitwise_and(image, image, mask=mask)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I07bWkBbFi50"
      },
      "outputs": [],
      "source": [
        "# def load_df_from_csv(csv_file):\n",
        "#     \"\"\"Load dataframe from csv file\"\"\"\n",
        "#     df = pd.read_csv(csv_file)\n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X29inA3QFi51",
        "outputId": "f0e008cf-6c14-46dc-e196-efe143f5ff16"
      },
      "outputs": [],
      "source": [
        "# df_train_cartoons = load_df_from_csv(\"../../data/cartoons_train.csv\")\n",
        "# df_train_cartoons.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn5f_yRWFi51",
        "outputId": "376b188b-ad99-4b30-8f03-3e34e65d36ba"
      },
      "outputs": [],
      "source": [
        "# df_train_images = load_df_from_csv(\"../../data/images_train.csv\")\n",
        "# df_train_images.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do-ZyYeZFi52"
      },
      "outputs": [],
      "source": [
        "# def read_image(image_path: str) -> np.ndarray:\n",
        "#     \"\"\"Read image from path\"\"\"\n",
        "#     image = cv2.imread(os.path.join(\"..\", \"..\", image_path))\n",
        "#     assert image is not None, \"Image is None\"\n",
        "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#     return image\n",
        "\n",
        "# def compute_image_histogram(image: np.ndarray) -> np.ndarray:\n",
        "#     \"\"\"Compute image histogram\"\"\"\n",
        "#     hist = cv2.calcHist([image], channels=[0, 1, 2], mask=None, histSize=[8, 8, 8], ranges=[0, 256, 0, 256, 0, 256])\n",
        "#     hist = cv2.normalize(hist, hist).flatten()\n",
        "#     return hist\n",
        "\n",
        "# def plot_image_and_its_histogram(image: np.ndarray) -> np.ndarray:\n",
        "#     \"\"\"Plot image histogram\"\"\"\n",
        "#     plt.figure(figsize=(10, 10))\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.imshow(image)\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(compute_image_histogram(image), color=\"blue\", linewidth=2, linestyle=\"-\", label=\"histogram\")\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUetLPqiFi52"
      },
      "outputs": [],
      "source": [
        "# # shuffle dataframe\n",
        "# df_train_cartoons = df_train_cartoons.sample(frac=1).reset_index(drop=True)\n",
        "# df_train_images = df_train_images.sample(frac=1).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxbn2vqsFi52",
        "outputId": "0e0f6dac-8081-41c5-815b-a5c4eccca4e6"
      },
      "outputs": [],
      "source": [
        "# for i in range(10):\n",
        "#     plot_image_and_its_histogram(read_image(df_train_cartoons.iloc[i][\"path\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cobmfTI7Fi53",
        "outputId": "ab391cb0-0e8c-4423-a4e8-170b970b4904"
      },
      "outputs": [],
      "source": [
        "# for i in range(10):\n",
        "#     plot_image_and_its_histogram(read_image(df_train_images.iloc[i][\"path\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GHWYzieFi53",
        "outputId": "ed8c5645-c554-4f82-8a12-51f47ae08800"
      },
      "outputs": [],
      "source": [
        "def compute_color_histograms(image: np.ndarray):\n",
        "    \"\"\"Compute color histograms\"\"\"\n",
        "    hist = cv2.calcHist([image], channels=[0, 1, 2], mask=None, histSize=[8, 8, 8], ranges=[0, 256, 0, 256, 0, 256])\n",
        "    hist = cv2.normalize(hist, hist).flatten()\n",
        "    return hist\n",
        "\n",
        "def plot_color_histogram_and_its_image(image: np.ndarray):\n",
        "    \"\"\"Plot color histogram and its image\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(compute_color_histograms(image), color=\"blue\", linewidth=2, linestyle=\"-\", label=\"histogram\")\n",
        "    plt.show()\n",
        "\n",
        "for i in range(10):\n",
        "    plot_color_histogram_and_its_image(read_image(df_train_cartoons.iloc[i][\"path\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YppxzZxlFi54"
      },
      "outputs": [],
      "source": [
        "def is_cartoon(image: np.ndarray, threshold: float = 0.98) -> bool:\n",
        "    \"\"\"Check if image is cartoon\"\"\"\n",
        "    image = cv2.resize(image, (256, 256))\n",
        "\n",
        "    color_blurred = cv2.bilateralFilter(image, 6, 250, 250)\n",
        "\n",
        "    # compare the colors from the original image to blurred one.\n",
        "    diffs = []\n",
        "    for k, _ in enumerate(('b', 'r', 'g')):\n",
        "        # print(f\"Comparing histogram for color {color}\")\n",
        "        real_histogram = cv2.calcHist(image, [k], None, [256], [0, 256])\n",
        "        color_histogram = cv2.calcHist(color_blurred, [k], None, [256], [0, 256])\n",
        "        diffs.append(cv2.compareHist(real_histogram, color_histogram, cv2.HISTCMP_CORREL))\n",
        "\n",
        "    return sum(diffs) / 3 > threshold\n",
        "\n",
        "def is_cartoon_color_count(image: np.ndarray, threshold: float = 0.3) -> bool:\n",
        "    image = cv2.resize(image, (1024, 1024))\n",
        "    # Find count of each color\n",
        "    color_count = {}\n",
        "    for row in image:\n",
        "        for item in row:\n",
        "            value = tuple(item)\n",
        "            if value not in color_count:\n",
        "                color_count[value] = 1\n",
        "            else:\n",
        "                color_count[value] += 1\n",
        "\n",
        "    # Identify the percent of the image that uses the top 512 colors\n",
        "    most_common_colors = sum([x[1] for x in sorted(color_count.items(), key=lambda pair: pair[1], reverse=True)[:512]])\n",
        "    return (most_common_colors / (1024 * 1024)) > threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBFJ-8rkFi54"
      },
      "outputs": [],
      "source": [
        "## with first method\n",
        "\n",
        "def do_grid_search_on_threshold(method: callable, thresholds: np.ndarray):\n",
        "    \"\"\"Do grid search on threshold\"\"\"\n",
        "    threshold_values = np.arange(0.1, 1.0, 0.1)\n",
        "    best_f1_score = 0\n",
        "    best_threshold = 0\n",
        "    for threshold in threshold_values:\n",
        "        tp = 0\n",
        "        tn = 0\n",
        "        fp = 0\n",
        "        fn = 0\n",
        "        for i in range(100):\n",
        "            image = read_image(df_train_cartoons.iloc[i][\"path\"])\n",
        "            if method(image, threshold):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "        for i in range(100):\n",
        "            image = read_image(df_train_images.iloc[i][\"path\"])\n",
        "            if method(image, threshold):\n",
        "                fp += 1\n",
        "            else:\n",
        "                tn += 1\n",
        "        f1_score = 2 * tp / (2 * tp + fp + fn)\n",
        "        if f1_score > best_f1_score:\n",
        "            best_f1_score = f1_score\n",
        "            best_threshold = threshold\n",
        "    print(f\"Best threshold: {best_threshold}\")\n",
        "    print(f\"Best f1 score: {best_f1_score}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_A_U_jjFi54",
        "outputId": "83a44666-c5b9-4f52-b263-b1803340764c"
      },
      "outputs": [],
      "source": [
        "## compare the two methods\n",
        "\n",
        "print(do_grid_search_on_threshold(is_cartoon, np.arange(0.1, 1.0, 0.1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIPvJxrHFi55",
        "outputId": "9612d370-cc8d-43f8-c322-3ef168d40205"
      },
      "outputs": [],
      "source": [
        "print(do_grid_search_on_threshold(is_cartoon_color_count, np.array([0.3])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "974wAEqbFi55",
        "outputId": "ca741b77-d034-48f5-e3bc-4facb575d90b"
      },
      "outputs": [],
      "source": [
        "one_test_img = read_image(df_train_images.iloc[0][\"path\"])\n",
        "\n",
        "def show_image(image: np.ndarray):\n",
        "    \"\"\"Show image\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "show_image(one_test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDsx5aLIFi55",
        "outputId": "65f482fc-e7e4-4bf0-a512-2a8f67de0277"
      },
      "outputs": [],
      "source": [
        "# Apply some Gaussian blur on the image\n",
        "img_gb = cv2.GaussianBlur(one_test_img, (7, 7) ,0)\n",
        "# Apply some Median blur on the image\n",
        "img_mb = cv2.medianBlur(img_gb, 5)\n",
        "# Apply a bilateral filer on the image\n",
        "img_bf = cv2.bilateralFilter(img_mb, 5, 80, 80)\n",
        "\n",
        "show_image(img_bf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwrzNaeRFi55",
        "outputId": "1dcb8439-22b8-452c-bf69-220b3eaf00d6"
      },
      "outputs": [],
      "source": [
        "img_lp_im = cv2.Laplacian(one_test_img, cv2.CV_8U, ksize=5)\n",
        "img_lp_gb = cv2.Laplacian(img_gb, cv2.CV_8U, ksize=5)\n",
        "img_lp_mb = cv2.Laplacian(img_mb, cv2.CV_8U, ksize=5)\n",
        "img_lp_al = cv2.Laplacian(img_bf, cv2.CV_8U, ksize=5)\n",
        "\n",
        "show_image(img_lp_im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Az-0SBFi56",
        "outputId": "b1ae4528-199f-4f05-8ed4-1bc535af3066"
      },
      "outputs": [],
      "source": [
        "# Convert the image to greyscale (1D)\n",
        "img_lp_im_grey = cv2.cvtColor(img_lp_im, cv2.COLOR_BGR2GRAY)\n",
        "img_lp_gb_grey = cv2.cvtColor(img_lp_gb, cv2.COLOR_BGR2GRAY)\n",
        "img_lp_mb_grey = cv2.cvtColor(img_lp_mb, cv2.COLOR_BGR2GRAY)\n",
        "img_lp_al_grey = cv2.cvtColor(img_lp_al, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "show_image(img_lp_im_grey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "864AAzK2Fi56"
      },
      "outputs": [],
      "source": [
        "# Manual image thresholding\n",
        "_, EdgeImage = cv2.threshold(one_test_img, 127, 255, cv2.THRESH_BINARY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjhzoHf8Fi56"
      },
      "outputs": [],
      "source": [
        "# Remove some additional noise\n",
        "blur_im = cv2.GaussianBlur(img_lp_im_grey, (5, 5), 0)\n",
        "blur_gb = cv2.GaussianBlur(img_lp_gb_grey, (5, 5), 0)\n",
        "blur_mb = cv2.GaussianBlur(img_lp_mb_grey, (5, 5), 0)\n",
        "blur_al = cv2.GaussianBlur(img_lp_al_grey, (5, 5), 0)\n",
        "# Apply a threshold (Otsu)\n",
        "_, tresh_im = cv2.threshold(blur_im, 245, 255,cv2.THRESH_BINARY +  cv2.THRESH_OTSU)\n",
        "_, tresh_gb = cv2.threshold(blur_gb, 245, 255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "_, tresh_mb = cv2.threshold(blur_mb, 245, 255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "_, tresh_al = cv2.threshold(blur_al, 245, 255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGU0unWnFi57",
        "outputId": "8e4a00b0-86e5-40b1-9e8c-9d6b9cb9a7e0"
      },
      "outputs": [],
      "source": [
        "# Invert the black and the white\n",
        "inverted_original = cv2.subtract(255, tresh_im)\n",
        "inverted_GaussianBlur = cv2.subtract(255, tresh_gb)\n",
        "inverted_MedianBlur = cv2.subtract(255, tresh_mb)\n",
        "inverted_Bilateral = cv2.subtract(255, tresh_al)\n",
        "\n",
        "show_image(inverted_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrSu72AkFi57"
      },
      "outputs": [],
      "source": [
        "# Reshape the image\n",
        "img_reshaped = one_test_img.reshape((-1,3))\n",
        "# convert to np.float32\n",
        "img_reshaped = np.float32(img_reshaped)\n",
        "# Set the Kmeans criteria\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "# Set the amount of K (colors)\n",
        "K = 8\n",
        "# Apply Kmeans\n",
        "_, label, center = cv2.kmeans(img_reshaped, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "# Covert it back to np.int8\n",
        "center = np.uint8(center)\n",
        "res = center[label.flatten()]\n",
        "# Reshape it back to an image\n",
        "img_Kmeans = res.reshape((one_test_img.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGAEPla5Fi57"
      },
      "outputs": [],
      "source": [
        "# Reduce the colors of the original image\n",
        "div = 64\n",
        "img_bins = one_test_img // div * div + div // 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg62UL8_Fi57",
        "outputId": "915aed8f-0d81-4abb-fc9b-35c13b786ad9"
      },
      "outputs": [],
      "source": [
        "# Convert the mask image back to color \n",
        "inverted_Bilateral = cv2.cvtColor(inverted_Bilateral, cv2.COLOR_GRAY2RGB)\n",
        "# Combine the edge image and the binned image\n",
        "cartoon_Bilateral = cv2.bitwise_and(inverted_Bilateral, img_bins)\n",
        "# Save the image\n",
        "cv2.imwrite('CartoonImage.png', cartoon_Bilateral)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "cartoonizing_with_cv2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7940bb513ed5abd51ff32721e45763594294f3b765a8837ebdd82e9a0a7d5879"
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 ('deepLearning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
